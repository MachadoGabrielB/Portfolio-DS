{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# df = pd.read_csv(\"../input/fe-course-data/concrete.csv\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.copy()\n",
    "# y = X.pop(\"CompressiveStrength\")\n",
    "\n",
    "# # Train and score baseline model\n",
    "# baseline = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\n",
    "# baseline_score = cross_val_score(\n",
    "#     baseline, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n",
    "# )\n",
    "# baseline_score = -1 * baseline_score.mean()\n",
    "\n",
    "# print(f\"MAE Baseline Score: {baseline_score:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.copy()\n",
    "# y = X.pop(\"CompressiveStrength\")\n",
    "\n",
    "# # Create synthetic features\n",
    "# X[\"FCRatio\"] = X[\"FineAggregate\"] / X[\"CoarseAggregate\"]\n",
    "# X[\"AggCmtRatio\"] = (X[\"CoarseAggregate\"] + X[\"FineAggregate\"]) / X[\"Cement\"]\n",
    "# X[\"WtrCmtRatio\"] = X[\"Water\"] / X[\"Cement\"]\n",
    "\n",
    "# # Train and score model on dataset with additional ratio features\n",
    "# model = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\n",
    "# score = cross_val_score(\n",
    "#     model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n",
    "# )\n",
    "# score = -1 * score.mean()\n",
    "\n",
    "# print(f\"MAE Score with Ratio Features: {score:.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First encountering a new dataset can sometimes feel overwhelming. You might be presented with hundreds or thousands of features without even a description to go by. Where do you even begin?\n",
    "\n",
    "A great first step is to construct a ranking with a feature utility metric, a function measuring associations between a feature and the target. Then you can choose a smaller set of the most useful features to develop initially and have more confidence that your time will be well spent.\n",
    "\n",
    "The metric we'll use is called \"mutual information\". Mutual information is a lot like correlation in that it measures a relationship between two quantities. The advantage of mutual information is that it can detect any kind of relationship, while correlation only detects linear relationships.\n",
    "\n",
    "Mutual information is a great general-purpose metric and especially useful at the start of feature development when you might not know what model you'd like to use yet. It is:\n",
    "\n",
    "easy to use and interpret,\n",
    "computationally efficient,\n",
    "theoretically well-founded,\n",
    "resistant to overfitting, and,\n",
    "able to detect any kind of relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn algorithm for MI treats discrete features differently from continuous features. Consequently, you need to tell it which are which. As a rule of thumb, anything that must have a float dtype is not discrete. Categoricals (object or categorial dtype) can be treated as discrete by giving them a label encoding. (You can review label encodings in our Categorical Variables lesson.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.copy()\n",
    "# y = X.pop(\"price\")\n",
    "\n",
    "# # Label encoding for categoricals\n",
    "# for colname in X.select_dtypes(\"object\"):\n",
    "#     X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# # All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "# discrete_features = X.dtypes == int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has two mutual information metrics in its feature_selection module: one for real-valued targets (mutual_info_regression) and one for categorical targets (mutual_info_classif). Our target, price, is real-valued. The next cell computes the MI scores for our features and wraps them up in a nice dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# def make_mi_scores(X, y, discrete_features):\n",
    "#     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "#     mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "#     mi_scores = mi_scores.sort_values(ascending=False)\n",
    "#     return mi_scores\n",
    "\n",
    "# mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "# mi_scores[::3]  # show a few features with their MI scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_mi_scores(scores):\n",
    "#     scores = scores.sort_values(ascending=True)\n",
    "#     width = np.arange(len(scores))\n",
    "#     ticks = list(scores.index)\n",
    "#     plt.barh(width, scores)\n",
    "#     plt.yticks(width, ticks)\n",
    "#     plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "# plt.figure(dpi=100, figsize=(8, 5))\n",
    "# plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.relplot(x=\"curb_weight\", y=\"price\", data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lmplot(x=\"horsepower\", y=\"price\", hue=\"fuel_type\", data=df);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x=\"BldgType\", y=\"SalePrice\", data=df, kind=\"boxen\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OverallQual     0.581262\n",
    "# Neighborhood    0.569813\n",
    "# GrLivArea       0.496909\n",
    "# YearBuilt       0.437939\n",
    "# GarageArea      0.415014\n",
    "# TotalBsmtSF     0.390280\n",
    "# GarageCars      0.381467\n",
    "# FirstFlrSF      0.368825\n",
    "# BsmtQual        0.364779\n",
    "# KitchenQual     0.326194\n",
    "# Name: MI Scores, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autos[\"stroke_ratio\"] = autos.stroke / autos.bore\n",
    "\n",
    "# autos[[\"stroke\", \"bore\", \"stroke_ratio\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Features ( True and False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"GiveWay\",\n",
    "#     \"Junction\", \"NoExit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n",
    "#     \"TrafficCalming\", \"TrafficSignal\"]\n",
    "# accidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1)\n",
    "\n",
    "# accidents[roadway_features + [\"RoadwayFeatures\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building-up or Breaking-down Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer[[\"Type\", \"Level\"]] = (  # Create two new features\n",
    "#     customer[\"Policy\"]           # from the Policy feature\n",
    "#     .str                         # through the string accessor\n",
    "#     .split(\" \", expand=True)     # by splitting on \" \"\n",
    "#                                  # and expanding the result into separate columns\n",
    "# )\n",
    "\n",
    "# customer[[\"Policy\", \"Type\", \"Level\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autos[\"make_and_style\"] = autos[\"make\"] + \"_\" + autos[\"body_style\"]\n",
    "# autos[[\"make\", \"body_style\", \"make_and_style\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Transforms \n",
    "(groupby, average, min, median, var, std, count, frequency )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer[\"AverageIncome\"] = (\n",
    "#     customer.groupby(\"State\")  # for each state\n",
    "#     [\"Income\"]                 # select the income\n",
    "#     .transform(\"mean\")         # and compute its mean\n",
    "# )\n",
    "\n",
    "# customer[[\"State\", \"Income\", \"AverageIncome\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer[\"StateFreq\"] = (\n",
    "#     customer.groupby(\"State\")\n",
    "#     [\"State\"]\n",
    "#     .transform(\"count\")\n",
    "#     / customer.State.count()\n",
    "# )\n",
    "\n",
    "# customer[[\"State\", \"StateFreq\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create splits\n",
    "# df_train = customer.sample(frac=0.5)\n",
    "# df_valid = customer.drop(df_train.index)\n",
    "\n",
    "# # Create the average claim amount by coverage type, on the training set\n",
    "# df_train[\"AverageClaim\"] = df_train.groupby(\"Coverage\")[\"ClaimAmount\"].transform(\"mean\")\n",
    "\n",
    "# # Merge the values into the validation set\n",
    "# df_valid = df_valid.merge(\n",
    "#     df_train[[\"Coverage\", \"AverageClaim\"]].drop_duplicates(),\n",
    "#     on=\"Coverage\",\n",
    "#     how=\"left\",\n",
    "# )\n",
    "\n",
    "# df_valid[[\"Coverage\", \"AverageClaim\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f you've discovered an interaction effect between a numeric feature and a categorical feature, you might want to model it explicitly using a one-hot encoding, like so:\n",
    "\n",
    "# # One-hot encode Categorical feature, adding a column prefix \"Cat\"\n",
    "# X_new = pd.get_dummies(df.Categorical, prefix=\"Cat\")\n",
    "\n",
    "# # Multiply row-by-row\n",
    "# X_new = X_new.mul(df.Continuous, axis=0)\n",
    "\n",
    "# # Join the new features to the feature set\n",
    "# X = X.join(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Clustering with K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review how the k-means algorithm learns the clusters and what that means for feature engineering. We'll focus on three parameters from scikit-learn's implementation: n_clusters, max_iter, and n_init."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to increase the max_iter for a large number of clusters or n_init for a complex dataset. Ordinarily though the only parameter you'll need to choose yourself is n_clusters (k, that is). The best partitioning for a set of features depends on the model you're using and what you're trying to predict, so it's best to tune it like any hyperparameter (through cross-validation, say)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create cluster feature\n",
    "# kmeans = KMeans(n_clusters=6)\n",
    "# X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "# X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n",
    "\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting Clustering Scatter Plot\n",
    "\n",
    "# sns.relplot(\n",
    "#     x=\"Longitude\", y=\"Latitude\", hue=\"Cluster\", data=X, height=6,\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting blox plot\n",
    "\n",
    "# X[\"MedHouseVal\"] = df[\"MedHouseVal\"]\n",
    "# sns.catplot(x=\"MedHouseVal\", y=\"Cluster\", data=X, kind=\"boxen\", height=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xy = X.copy()\n",
    "# Xy[\"Cluster\"] = Xy.Cluster.astype(\"category\")\n",
    "# Xy[\"SalePrice\"] = y\n",
    "# sns.relplot(\n",
    "#     x=\"value\", y=\"SalePrice\", hue=\"Cluster\", col=\"variable\",\n",
    "#     height=4, aspect=1, facet_kws={'sharex': False}, col_wrap=3,\n",
    "#     data=Xy.melt(\n",
    "#         value_vars=features, id_vars=[\"SalePrice\", \"Cluster\"],\n",
    "#     ),\n",
    "# );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways you could use PCA for feature engineering.\n",
    "\n",
    "The first way is to use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create -- a product of 'Height' and 'Diameter' if 'Size' is important, say, or a ratio of 'Height' and 'Diameter' if Shape is important. You could even try clustering on one or more of the high-scoring components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA basically gives you direct access to the correlational structure of your data. You'll no doubt come up with applications of your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Best Practices\n",
    "There are a few things to keep in mind when applying PCA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA only works with numeric features, like continuous quantities or counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is sensitive to scale. It's good practice to standardize your data before applying PCA, unless you know you have good reason not to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider removing or constraining outliers, since they can have an undue influence on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# from IPython.display import display\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\n",
    "# plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "# plt.rc(\"figure\", autolayout=True)\n",
    "# plt.rc(\n",
    "#     \"axes\",\n",
    "#     labelweight=\"bold\",\n",
    "#     labelsize=\"large\",\n",
    "#     titleweight=\"bold\",\n",
    "#     titlesize=14,\n",
    "#     titlepad=10,\n",
    "# )\n",
    "\n",
    "\n",
    "# def plot_variance(pca, width=8, dpi=100):\n",
    "#     # Create figure\n",
    "#     fig, axs = plt.subplots(1, 2)\n",
    "#     n = pca.n_components_\n",
    "#     grid = np.arange(1, n + 1)\n",
    "#     # Explained variance\n",
    "#     evr = pca.explained_variance_ratio_\n",
    "#     axs[0].bar(grid, evr)\n",
    "#     axs[0].set(\n",
    "#         xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "#     )\n",
    "#     # Cumulative Variance\n",
    "#     cv = np.cumsum(evr)\n",
    "#     axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "#     axs[1].set(\n",
    "#         xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "#     )\n",
    "#     # Set up figure\n",
    "#     fig.set(figwidth=8, dpi=100)\n",
    "#     return axs\n",
    "\n",
    "# def make_mi_scores(X, y, discrete_features):\n",
    "#     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "#     mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "#     mi_scores = mi_scores.sort_values(ascending=False)\n",
    "#     return mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [\"highway_mpg\", \"engine_size\", \"horsepower\", \"curb_weight\"]\n",
    "\n",
    "# X = df.copy()\n",
    "# y = X.pop('price')\n",
    "# X = X.loc[:, features]\n",
    "\n",
    "# # Standardize\n",
    "# X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Create principal components\n",
    "# pca = PCA()\n",
    "# X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# # Convert to dataframe\n",
    "# component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "# X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "# X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadings = pd.DataFrame(\n",
    "#     pca.components_.T,  # transpose the matrix of loadings\n",
    "#     columns=component_names,  # so the columns are the principal components\n",
    "#     index=X.columns,  # and the rows are the original features\n",
    "# )\n",
    "# loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at explained variance\n",
    "# plot_variance(pca);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi_scores = make_mi_scores(X_pca, y, discrete_features=False)\n",
    "# mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show dataframe sorted by PC3\n",
    "# idx = X_pca[\"PC3\"].sort_values(ascending=False).index\n",
    "# cols = [\"make\", \"body_style\", \"horsepower\", \"curb_weight\"]\n",
    "# df.loc[idx, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"sports_or_wagon\"] = X.curb_weight / X.horsepower\n",
    "# sns.regplot(x=\"sports_or_wagon\", y='price', data=df, order=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup feedback system\n",
    "# from learntools.core import binder\n",
    "# binder.bind(globals())\n",
    "# from learntools.feature_engineering_new.ex5 import *\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# # Set Matplotlib defaults\n",
    "# plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "# plt.rc(\"figure\", autolayout=True)\n",
    "# plt.rc(\n",
    "#     \"axes\",\n",
    "#     labelweight=\"bold\",\n",
    "#     labelsize=\"large\",\n",
    "#     titleweight=\"bold\",\n",
    "#     titlesize=14,\n",
    "#     titlepad=10,\n",
    "# )\n",
    "\n",
    "\n",
    "# def apply_pca(X, standardize=True):\n",
    "#     # Standardize\n",
    "#     if standardize:\n",
    "#         X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "#     # Create principal components\n",
    "#     pca = PCA()\n",
    "#     X_pca = pca.fit_transform(X)\n",
    "#     # Convert to dataframe\n",
    "#     component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "#     X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "#     # Create loadings\n",
    "#     loadings = pd.DataFrame(\n",
    "#         pca.components_.T,  # transpose the matrix of loadings\n",
    "#         columns=component_names,  # so the columns are the principal components\n",
    "#         index=X.columns,  # and the rows are the original features\n",
    "#     )\n",
    "#     return pca, X_pca, loadings\n",
    "\n",
    "\n",
    "# def plot_variance(pca, width=8, dpi=100):\n",
    "#     # Create figure\n",
    "#     fig, axs = plt.subplots(1, 2)\n",
    "#     n = pca.n_components_\n",
    "#     grid = np.arange(1, n + 1)\n",
    "#     # Explained variance\n",
    "#     evr = pca.explained_variance_ratio_\n",
    "#     axs[0].bar(grid, evr)\n",
    "#     axs[0].set(\n",
    "#         xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "#     )\n",
    "#     # Cumulative Variance\n",
    "#     cv = np.cumsum(evr)\n",
    "#     axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "#     axs[1].set(\n",
    "#         xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "#     )\n",
    "#     # Set up figure\n",
    "#     fig.set(figwidth=8, dpi=100)\n",
    "#     return axs\n",
    "\n",
    "\n",
    "# def make_mi_scores(X, y):\n",
    "#     X = X.copy()\n",
    "#     for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "#         X[colname], _ = X[colname].factorize()\n",
    "#     # All discrete features should now have integer dtypes\n",
    "#     discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "#     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "#     mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "#     mi_scores = mi_scores.sort_values(ascending=False)\n",
    "#     return mi_scores\n",
    "\n",
    "\n",
    "# def score_dataset(X, y, model=XGBRegressor()):\n",
    "#     # Label encoding for categoricals\n",
    "#     for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "#         X[colname], _ = X[colname].factorize()\n",
    "#     # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "#     score = cross_val_score(\n",
    "#         model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "#     )\n",
    "#     score = -1 * score.mean()\n",
    "#     score = np.sqrt(score)\n",
    "#     return score\n",
    "\n",
    "\n",
    "# df = pd.read_csv(\"../input/fe-course-data/ames.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct:\n",
    "\n",
    "The first component, PC1, seems to be a kind of \"size\" component, similar to what we saw in the tutorial: all of the features have the same sign (positive), indicating that this component is describing a contrast between houses having large values and houses having small values for these features.\n",
    "\n",
    "The interpretation of the third component PC3 is a little trickier. The features GarageArea and YearRemodAdd both have near-zero loadings, so let's ignore those. This component is mostly about TotalBsmtSF and GrLivArea. It describes a contrast between houses with a lot of living area but small (or non-existant) basements, and the opposite: small houses with large basements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A target encoding is any kind of encoding that replaces a feature's categories with some number derived from the target.\n",
    "\n",
    "A simple and effective version is to apply a group aggregation from Lesson 3, like the mean. Using the Automobiles dataset, this computes the average price of each vehicle's make:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of target encoding is sometimes called a mean encoding. Applied to a binary target, it's also called bin counting. (Other names you might come across include: likelihood encoding, impact encoding, and leave-one-out encoding.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An encoding like this presents a couple of problems, however. First are unknown categories. Target encodings create a special risk of overfitting, which means they need to be trained on an independent \"encoding\" split. When you join the encoding to future splits, Pandas will fill in missing values for any categories not present in the encoding split. These missing values you would have to impute somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second are rare categories. When a category only occurs a few times in the dataset, any statistics calculated on its group are unlikely to be very accurate. In the Automobiles dataset, the mercurcy make only occurs once. The \"mean\" price we calculated is just the price of that one vehicle, which might not be very representative of any Mercuries we might see in the future. Target encoding rare categories can make overfitting more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solution to these problems is to add smoothing. The idea is to blend the in-category average with the overall average. Rare categories get less weight on their category average, while missing categories just get the overall average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pseudocode:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = weight * in_category + (1 - weight) * overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where weight is a value between 0 and 1 calculated from the category frequency.\n",
    "\n",
    "An easy way to determine the value for weight is to compute an m-estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight = n / (n + m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where n is the total number of times that category occurs in the data. The parameter m determines the \"smoothing factor\". Larger values of m put more weight on the overall estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Automobiles dataset there are three cars with the make chevrolet. If you chose m=2.0, then the chevrolet category would be encoded with 60% of the average Chevrolet price plus 40% of the overall average price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chevrolet = 0.6 * 6000.00 + 0.4 * 13285.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When choosing a value for m, consider how noisy you expect the categories to be. Does the price of a vehicle vary a great deal within each make? Would you need a lot of data to get good estimates? If so, it could be better to choose a larger value for m; if the average price for each make were relatively stable, a smaller value could be okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Cases for Target Encoding\n",
    "Target encoding is great for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-cardinality features: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain-motivated features: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.copy()\n",
    "# y = X.pop('Rating')\n",
    "\n",
    "# X_encode = X.sample(frac=0.25)\n",
    "# y_encode = y[X_encode.index]\n",
    "# X_pretrain = X.drop(X_encode.index)\n",
    "# y_train = y[X_pretrain.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from category_encoders import MEstimateEncoder\n",
    "\n",
    "# # Create the encoder instance. Choose m to control noise.\n",
    "# encoder = MEstimateEncoder(cols=[\"Zipcode\"], m=5.0)\n",
    "\n",
    "# # Fit the encoder on the encoding split.\n",
    "# encoder.fit(X_encode, y_encode)\n",
    "\n",
    "# # Encode the Zipcode column to create the final training data\n",
    "# X_train = encoder.transform(X_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoding split\n",
    "# X_encode = df.sample(frac=0.20, random_state=0)\n",
    "# y_encode = X_encode.pop(\"SalePrice\")\n",
    "\n",
    "# # Training split\n",
    "# X_pretrain = df.drop(X_encode.index)\n",
    "# y_train = X_pretrain.pop(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.copy()\n",
    "# y = X.pop(\"SalePrice\")\n",
    "# score_base = score_dataset(X, y)\n",
    "# score_new = score_dataset(X_train, y_train)\n",
    "\n",
    "# print(f\"Baseline Score: {score_base:.4f} RMSLE\")\n",
    "# print(f\"Score with Encoding: {score_new:.4f} RMSLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your understanding of how mean-encoding works, can you explain how XGBoost was able to get an almost a perfect fit after mean-encoding the count feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Count never has any duplicate values, the mean-encoded Count is essentially an exact copy of the target. In other words, mean-encoding turned a completely meaningless feature into a perfect feature.\n",
    "\n",
    "Now, the only reason this worked is because we trained XGBoost on the same set we used to train the encoder. If we had used a hold-out set instead, none of this \"fake\" encoding would have transferred to the training data.\n",
    "\n",
    "The lesson is that when using a target encoder it's very important to use separate data sets for training the encoder and training the model. Otherwise the results can be very disappointing!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
